{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124ff4df-8cc7-4d05-8584-d5b83d80bf3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:26:42.820882Z",
     "iopub.status.busy": "2026-01-04T13:26:42.820522Z",
     "iopub.status.idle": "2026-01-04T13:26:42.828546Z",
     "shell.execute_reply": "2026-01-04T13:26:42.826320Z",
     "shell.execute_reply.started": "2026-01-04T13:26:42.820847Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Callable\n",
    "from dotenv import load_dotenv\n",
    "from ollama import chat\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fceb7201-50d3-4c23-a156-1c6de5a4c0fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:26:46.772155Z",
     "iopub.status.busy": "2026-01-04T13:26:46.771744Z",
     "iopub.status.idle": "2026-01-04T13:26:46.780780Z",
     "shell.execute_reply": "2026-01-04T13:26:46.778970Z",
     "shell.execute_reply.started": "2026-01-04T13:26:46.772127Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "NUM_RUNS_TIMES = 5\n",
    "\n",
    "# DATA_FILES: List[str] = [\n",
    "#     os.path.join(os.path.dirname(__file__), \"data\", \"api_docs.txt\"),\n",
    "# ]\n",
    "BASE_DIR = Path().resolve()   # folder chá»©a notebook\n",
    "\n",
    "DATA_FILES = [\n",
    "    BASE_DIR / \"data\" / \"api_docs.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1177145b-c011-400a-8e3d-dec5ac97fec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:27:15.189532Z",
     "iopub.status.busy": "2026-01-04T13:27:15.189126Z",
     "iopub.status.idle": "2026-01-04T13:27:15.196972Z",
     "shell.execute_reply": "2026-01-04T13:27:15.195176Z",
     "shell.execute_reply.started": "2026-01-04T13:27:15.189505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/jupyter/Google-Skill-VertexAI/modern-software-dev-assignments/week1/data/api_docs.txt')]\n"
     ]
    }
   ],
   "source": [
    "print(DATA_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a956371c273af5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T13:29:34.724152Z",
     "iopub.status.busy": "2026-01-04T13:29:34.723826Z",
     "iopub.status.idle": "2026-01-04T13:29:34.906751Z",
     "shell.execute_reply": "2026-01-04T13:29:34.904299Z",
     "shell.execute_reply.started": "2026-01-04T13:29:34.724126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test 1 of 5\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 120\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mtest_your_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYOUR_SYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYOUR_CONTEXT_PROVIDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m, in \u001b[0;36mtest_your_prompt\u001b[0;34m(system_prompt, context_provider)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_RUNS_TIMES):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_RUNS_TIMES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1:8b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    105\u001b[0m     code \u001b[38;5;241m=\u001b[39m extract_code_block(output_text)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ollama/_client.py:365\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    319\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    320\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ollama/_client.py:189\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ollama/_client.py:135\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m--> 135\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "def load_corpus_from_files(paths: List[str]) -> List[str]:\n",
    "    corpus: List[str] = []\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    corpus.append(f.read())\n",
    "            except Exception as exc:\n",
    "                corpus.append(f\"[load_error] {p}: {exc}\")\n",
    "        else:\n",
    "            corpus.append(f\"[missing_file] {p}\")\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# Load corpus from external files (simple API docs). If missing, fall back to inline snippet\n",
    "CORPUS: List[str] = load_corpus_from_files(DATA_FILES)\n",
    "\n",
    "QUESTION = (\n",
    "    \"Write a Python function `fetch_user_name(user_id: str, api_key: str) -> str` that calls the documented API \"\n",
    "    \"to fetch a user by id and returns only the user's name as a string.\"\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Fill this in!\n",
    "# YOUR_SYSTEM_PROMPT = \"\"\n",
    "YOUR_SYSTEM_PROMPT = \"\"\"You are a Python code generator who uses provided API documentation to write functions.\n",
    "\n",
    "Rules:\n",
    "1. Use ONLY the context information provided\n",
    "2. Follow the exact API specifications shown in the documentation\n",
    "3. Include necessary imports\n",
    "4. Handle authentication exactly as documented\n",
    "5. Return only what the task specifies\n",
    "\n",
    "Generate clean, functional Python code based on the given API documentation.\"\"\"\n",
    "\n",
    "\n",
    "# For this simple example\n",
    "# For this coding task, validate by required snippets rather than exact string\n",
    "REQUIRED_SNIPPETS = [\n",
    "    \"def fetch_user_name(\",\n",
    "    \"requests.get\",\n",
    "    \"/users/\",\n",
    "    \"X-API-Key\",\n",
    "    \"return\",\n",
    "]\n",
    "\n",
    "\n",
    "def YOUR_CONTEXT_PROVIDER(corpus: List[str]) -> List[str]:\n",
    "    \"\"\"TODO: Select and return the relevant subset of documents from CORPUS for this task.\n",
    "\n",
    "    For example, return [] to simulate missing context, or [corpus[0]] to include the API docs.\n",
    "    \"\"\"\n",
    "    # Return all available corpus documents to provide full API documentation context\n",
    "    # return []\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def make_user_prompt(question: str, context_docs: List[str]) -> str:\n",
    "    if context_docs:\n",
    "        context_block = \"\\n\".join(f\"- {d}\" for d in context_docs)\n",
    "    else:\n",
    "        context_block = \"(no context provided)\"\n",
    "    return (\n",
    "        f\"Context (use ONLY this information):\\n{context_block}\\n\\n\"\n",
    "        f\"Task: {question}\\n\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"- Use the documented Base URL and endpoint.\\n\"\n",
    "        \"- Send the documented authentication header.\\n\"\n",
    "        \"- Raise for non-200 responses.\\n\"\n",
    "        \"- Return only the user's name string.\\n\\n\"\n",
    "        \"Output: A single fenced Python code block with the function and necessary imports.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_code_block(text: str) -> str:\n",
    "    \"\"\"Extract the last fenced Python code block, or any fenced code block, else return text.\"\"\"\n",
    "    # Try ```python ... ``` first\n",
    "    m = re.findall(r\"```python\\n([\\s\\S]*?)```\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m[-1].strip()\n",
    "    # Fallback to any fenced code block\n",
    "    m = re.findall(r\"```\\n([\\s\\S]*?)```\", text)\n",
    "    if m:\n",
    "        return m[-1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def test_your_prompt(system_prompt: str, context_provider: Callable[[List[str]], List[str]]) -> bool:\n",
    "    \"\"\"Run up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.\"\"\"\n",
    "    context_docs = context_provider(CORPUS)\n",
    "    user_prompt = make_user_prompt(QUESTION, context_docs)\n",
    "\n",
    "    for idx in range(NUM_RUNS_TIMES):\n",
    "        print(f\"Running test {idx + 1} of {NUM_RUNS_TIMES}\")\n",
    "        response = chat(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            options={\"temperature\": 0.0},\n",
    "        )\n",
    "        output_text = response.message.content\n",
    "        code = extract_code_block(output_text)\n",
    "        missing = [s for s in REQUIRED_SNIPPETS if s not in code]\n",
    "        if not missing:\n",
    "            print(output_text)\n",
    "            print(\"SUCCESS\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Missing required snippets:\")\n",
    "            for s in missing:\n",
    "                print(f\"  - {s}\")\n",
    "            print(\"Generated code:\\n\" + code)\n",
    "    return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_your_prompt(YOUR_SYSTEM_PROMPT, YOUR_CONTEXT_PROVIDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35b367-705d-443e-baf4-93dc22f569b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m137",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m137"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
